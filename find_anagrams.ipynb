{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import json\n",
    "import random\n",
    "import csv\n",
    "from math import factorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "funnyWords1 = {}\n",
    "funnyWords = {}\n",
    "\n",
    "with open('funny_words.csv', ) as f:\n",
    "    reader = csv.reader(f, delimiter = ',')\n",
    "    for row in reader:\n",
    "        if row[0] == 'word':\n",
    "            continue\n",
    "        funnyWords1[float(row[1])] = row[0]\n",
    "\n",
    "for key in funnyWords1:\n",
    "    if key > 2.8:\n",
    "        funnyWords[funnyWords1[key]] = key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('common_words_no_names.txt') as f:\n",
    "    common_words = f.read().splitlines()\n",
    "g = t.Generator().manual_seed(214743647)\n",
    "numerated = {}\n",
    "for i in range(len(common_words)):\n",
    "    numerated[common_words[i]] = i\n",
    "\n",
    "common_words = set(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2', clean_up_tokenization_spaces = False, dtype = t.float16)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob(text):\n",
    "    if ' ' not in text:\n",
    "        return t.tensor(0)\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    prob = 1\n",
    "    # with t.no_grad():\n",
    "    #     outputs = model(input_ids)\n",
    "    #     logits = outputs.logits\n",
    "    # print(logits.shape)\n",
    "    logits = t.randn(1,list(t.Tensor.size(input_ids))[1] - 1, 50257)\n",
    "    for i in range(list(t.Tensor.size(input_ids))[1] - 1):\n",
    "        next_token_logits = logits[:, i, :]\n",
    "        probabilities = F.softmax(next_token_logits, dim=-1)\n",
    "        next_token_id = input_ids[0][i + 1].item()\n",
    "        prob *= probabilities[0][next_token_id]\n",
    "    if prob != 1:\n",
    "        return prob\n",
    "    else:\n",
    "        return t.tensor(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A:\n",
    "    def __init__(self):\n",
    "        self.common_words = open('common_words_no_names.txt').read().split()\n",
    "        with open('memo_dict.json', 'r') as f:\n",
    "            self.memoDict = json.load(f)\n",
    "\n",
    "    def save_anagrams(self):\n",
    "        open('memo_dict.json', 'r').close()\n",
    "        with open('memo_dict.json', 'w') as f:\n",
    "            json.dump(self.memoDict, f)\n",
    "\n",
    "    def anagrams_helper(self, word):\n",
    "        if word == '':\n",
    "            return ['']\n",
    "        if word in self.memoDict:\n",
    "            return self.memoDict[word]\n",
    "        dict = {}\n",
    "        for w in common_words:\n",
    "            wordHusk = word\n",
    "            included = True\n",
    "            for c in w:\n",
    "                if c not in wordHusk:\n",
    "                    included = False\n",
    "                    break\n",
    "                else:\n",
    "                    i = wordHusk.index(c)\n",
    "                    wordHusk = wordHusk[:i] + wordHusk[i+1:]\n",
    "            if included:\n",
    "                dict[w] = wordHusk\n",
    "        if len(word) < self.length - 2:     # adds small words to memoization dictionary for speed\n",
    "                self.memoDict[word] = [w + ' ' + g for w in list(dict.keys()) for g in self.anagrams_helper(dict[w])]\n",
    "        return [w + ' ' + g for w in list(dict.keys()) for g in self.anagrams_helper(dict[w])]\n",
    "\n",
    "    def anagrams(self, input_word):\n",
    "        self.length = len(input_word)\n",
    "        input_word = input_word.lower()\n",
    "        word = ''\n",
    "        for c in input_word:\n",
    "            if c in 'abcdefghijklmnopqrstuvwxyz':\n",
    "                word += c\n",
    "        words = self.anagrams_helper(input_word.lower())\n",
    "        for w in words:\n",
    "            w = w.split(' ')\n",
    "        words.sort(key = len)\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = A()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8349186"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_word = 'machinelearning'\n",
    "words = g.anagrams(input_word)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letter_likelihood(word):\n",
    "    word = word.lower()\n",
    "    l = 1.12*(word.count('e')) + .85*(word.count('a')) +.76*(word.count('r')) +.75*(word.count('i')) +.2*(word.count('o')) +.7*(word.count('t')) +.67*(word.count('n')) +.57*(word.count('s')) +.55*(word.count('l')) +.45*(word.count('c')) +.36*(word.count('u')) +.34*(word.count('d')) +.32*(word.count('p')) +.3*(word.count('m')) +.3*(word.count('h')) +.25*(word.count('g')) +.21*(word.count('b')) +.18*(word.count('f')) + .18*(word.count('y')) +.13*(word.count('w')) +.11*(word.count('k')) +.1*(word.count('v')) +.03*(word.count('x')) +.027*(word.count('z')) +.02*(word.count('j')) +.02*(word.count('q'))\n",
    "    return round(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "slice = int(2000000 / factorial(letter_likelihood(input_word)))\n",
    "words1 = words[:slice]\n",
    "print(len(words1))\n",
    "\n",
    "print(\"\\n\".join(sorted(words1, reverse = True, key = get_prob)))\n",
    "# print(\"\\n\".join(words[slice:]))\n",
    "\n",
    "# 11:15\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
