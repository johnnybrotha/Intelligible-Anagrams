{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from datasets import load_dataset\n",
    "#ds = load_dataset(\"pszemraj/simple_wikipedia\")\n",
    "import torch as t\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('common_words_no_names.txt') as f:\n",
    "    common_words = f.read().splitlines()\n",
    "g = t.Generator().manual_seed(214743647)\n",
    "numerated = {}\n",
    "for i in range(len(common_words)):\n",
    "    numerated[common_words[i]] = i\n",
    "\n",
    "lookup = t.randn((len(common_words), 10), generator = g)\n",
    "lookup1 = lookup.clone().detach()\n",
    "common_words = set(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(ds['train']))):\n",
    "    paragraph = ds['train'][i]['text'].lower().replace('\\n', ' ').split('. ')\n",
    "    paragraph2 = []\n",
    "    for p in paragraph:\n",
    "        if len(p.split('? ')) > 1:\n",
    "            for g in p.split('? '):\n",
    "                paragraph2.append(g)\n",
    "        else:\n",
    "            paragraph2.append(p)\n",
    "\n",
    "    sentences = []\n",
    "\n",
    "    for p in paragraph2:\n",
    "        if len(p.split('! ')) > 1:\n",
    "            for g in p.split('! '):\n",
    "                sentences.append(g.split(' '))\n",
    "        else:\n",
    "            sentences.append(p.split(' '))\n",
    "\n",
    "    for sentence in sentences:\n",
    "        for word1 in sentence:\n",
    "            for word2 in sentence:\n",
    "                if word1 != word2 and word1 in common_words and word2 in common_words and word1 != '' and word2 != '':  # might work better to use all possible words not just common\n",
    "                    lookup[numerated[word1]] = t.add(lookup[numerated[word1]], 0.05*t.sub(lookup[numerated[word2]], lookup[numerated[word1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dotNorm(a, b):\n",
    "    return t.dot(a, b) / (t.norm(a) * t.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotNorm(t.tensor([0,1], dtype=t.float), t.tensor([1,0], dtype=t.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotNorm(lookup[numerated['the']], lookup[numerated['cut']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0004,  0.0130,  0.0280,  0.0006,  0.0072,  0.0094, -0.0072, -0.0063,\n",
       "        -0.0121,  0.0210])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup[numerated['the']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0004,  0.0130,  0.0280,  0.0006,  0.0072,  0.0095, -0.0072, -0.0063,\n",
      "        -0.0121,  0.0210])\n"
     ]
    }
   ],
   "source": [
    "for v in lookup:\n",
    "    if t.equal(v,lookup[numerated['cat']]):\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9881740262558316"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "changed = 0\n",
    "count = 0\n",
    "for i in range(len(common_words)):\n",
    "    if not lookup1[i].equal(lookup[i]):\n",
    "        changed += 1\n",
    "    count += 1\n",
    "\n",
    "changed / count    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"['joan', 'than']\",\n",
       " \"['an', 'jo', 'than']\",\n",
       " \"['ha', 'tn', 'joan']\",\n",
       " \"['an', 'ha', 'jo', 'tn']\"]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_words = open('common_words_no_names.txt').read().split()\n",
    "\n",
    "# add memoization\n",
    "\n",
    "def anagwams(word):\n",
    "    if word == '':\n",
    "        return ['']\n",
    "    dict = {}\n",
    "    for w in common_words:\n",
    "        wordHusk = word\n",
    "        included = True\n",
    "        for c in w:\n",
    "            if c not in wordHusk:\n",
    "                included = False\n",
    "                break\n",
    "            else:\n",
    "                i = wordHusk.index(c)\n",
    "                wordHusk = wordHusk[:i] + wordHusk[i+1:]\n",
    "        if included:\n",
    "            dict[w] = wordHusk\n",
    "    return [w + ' ' + g for w in list(dict.keys()) for g in anagwams(dict[w])]\n",
    "\n",
    "def anagrams(word):\n",
    "    words = anagwams(word)\n",
    "    uniqueWords = set([])\n",
    "    for w in words:\n",
    "        Wsplit = sorted(sorted(w.split(' ')), key = len)\n",
    "        uniqueWords.add(str(Wsplit[1:]))\n",
    "    return list(uniqueWords)\n",
    "\n",
    "out = anagrams('jonathan')\n",
    "sorted(out, key = len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af3123bb143e4c5586dc6fd2a05e3fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d54f85d57394cda8daa95e99b704347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e3839493392440b8eb324dccd61ef66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87067bd2300d4a23b1f4a78cb35dd55d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cae3e683c4246b1b98ca5f717eb1260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "925c2d01992443429e3e83e09f5078af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2634af00c874aedbc07b92cd8ed43d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JonathanG-B/Desktop/Projects/anagrams/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = 'gpt2'  # You can use 'gpt2-medium', 'gpt2-large', etc.\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: were, Probability: 0.2231\n",
      "Token: fell, Probability: 0.1174\n",
      "Token: collapsed, Probability: 0.0579\n",
      "Token: came, Probability: 0.0547\n",
      "Token: of, Probability: 0.0371\n",
      "Token: are, Probability: 0.0366\n",
      "Token: went,, Probability: 0.0352\n",
      "Token: opened, Probability: 0.0249\n",
      "Token: had, Probability: 0.0243\n"
     ]
    }
   ],
   "source": [
    "text = \"Before the towers\"\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "with t.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Get logits for the last token in the input sequence\n",
    "next_token_logits = logits[:, -1, :]\n",
    "\n",
    "# Convert logits to probabilities\n",
    "probabilities = F.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "# Get the top 10 most probable tokens\n",
    "top_k = 10\n",
    "top_k_probs, top_k_indices = t.topk(probabilities, top_k)\n",
    "\n",
    "top_k_tokens = tokenizer.decode(top_k_indices[0]).split()\n",
    "top_k_probabilities = top_k_probs[0].tolist()\n",
    "\n",
    "for token, prob in zip(top_k_tokens, top_k_probabilities):\n",
    "    print(f\"Token: {token}, Probability: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07800176739692688\n",
      "2.6427536795381457e-05\n"
     ]
    }
   ],
   "source": [
    "tokens = 'Before the towers'\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "with t.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits\n",
    "\n",
    "for i in range(list(t.Tensor.size(input_ids))[1] - 1):\n",
    "    next_token_logits = logits[:, i, :]\n",
    "    probabilities = F.softmax(next_token_logits, dim=-1)\n",
    "    next_token_id = input_ids[0][i + 1].item()\n",
    "\n",
    "    retro_prob = probabilities[0][next_token_id]\n",
    "    print(retro_prob.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1324e-06, 1.5012e-06, 2.4056e-08,  ..., 6.6154e-11, 6.8069e-07,\n",
       "        2.3844e-07])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.Tensor.size(probabilities)\n",
    "probabilities[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
